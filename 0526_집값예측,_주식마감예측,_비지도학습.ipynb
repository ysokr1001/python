{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0526-집값예측, 주식마감예측, 비지도학습",
      "provenance": [],
      "mount_file_id": "1KBmE6WV0uua7u4Z8qKcIo7ci1lwPloZ6",
      "authorship_tag": "ABX9TyPT7CPZJ30BErWBHsjrnS1B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ysokr1001/python/blob/main/0526_%EC%A7%91%EA%B0%92%EC%98%88%EC%B8%A1%2C_%EC%A3%BC%EC%8B%9D%EB%A7%88%EA%B0%90%EC%98%88%EC%B8%A1%2C_%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3lxjQhH_Bgt"
      },
      "source": [
        "#집값 예측 - 선형회귀 실행 \n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "seed= 0\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "df= pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/housing.csv\", delim_whitespace=True, header=None)\n",
        "\n",
        "dataset = df.values\n",
        "X = dataset[:, 0:13]\n",
        "Y = dataset[:, 13]\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(30, input_dim=13, activation='relu'))\n",
        "model.add(Dense(6, activation='relu'))\n",
        "model.add(Dense(1)) \n",
        "\n",
        "model.compile(loss = 'mean_squared_error', optimizer='adam')\n",
        "model.fit(X_train, Y_train, epochs=200, batch_size=10)\n",
        "\n",
        "Y_prediction = model.predict(X_test).flatten()\n",
        "for i in range(10):\n",
        "  label = Y_test[i]\n",
        "  prediction = Y_prediction[i]\n",
        "  print(\"실제가격: {:.3f}, 예상가격: {:.3f}\".format(label, prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vpPTqNHKCd7y",
        "outputId": "f28c767e-5bee-4780-cda7-6121bc5986c5"
      },
      "source": [
        "#집값 예측 - 집값 예측 자동 중단 및 Graph 적용\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "seed= 0\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "df_pre= pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/housing.csv\", delim_whitespace=True, header=None)\n",
        "df = df_pre.sample(frac=1)\n",
        "\n",
        "dataset = df.values\n",
        "X = dataset[:, 0:13]\n",
        "Y = dataset[:, 13]\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(30, input_dim=13, activation='relu'))\n",
        "model.add(Dense(6, activation='relu'))\n",
        "model.add(Dense(1)) \n",
        "\n",
        "model.compile(loss = 'mean_squared_error', optimizer='adam')\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "early_stopping_callback = EarlyStopping(monitor = \"val_loss\", patience=50)\n",
        "\n",
        "modelpath = \"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
        "checkpointer = ModelCheckpoint(filepath=modelpath, moniter='val_loss', verbose = 1, save_best_only=True)\n",
        "\n",
        "\n",
        "history = model.fit(X_train, Y_train, validation_split=0.33, epochs=2000, batch_size=10, callbacks=[early_stopping_callback, checkpointer])\n",
        "\n",
        "hist = pd.DataFrame(history.history)\n",
        "print(hist.tail())\n",
        "\n",
        "\n",
        "y_vloss = history.history['val_loss']\n",
        "y_loss = history.history['loss']\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "x_len = np.arange(len(y_loss))\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(x_len, y_vloss, \"o\", c='violet', markersize=3, label='val_loss')\n",
        "plt.plot(x_len, y_loss, \"o\", c='springgreen', markersize=3, label='loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Y_prediction = model.predict(X_test).flatten()\n",
        "for i in range(10):\n",
        "  label = Y_test[i]\n",
        "  prediction = Y_prediction[i]\n",
        "  print(\"실제가격: {:.3f}, 예상가격: {:.3f}\".format(label, prediction))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "24/24 [==============================] - 0s 6ms/step - loss: 10635.3115 - val_loss: 3653.8191\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3653.81909, saving model to ./model/01-3653.8191.hdf5\n",
            "Epoch 2/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 1417.0900 - val_loss: 684.9082\n",
            "\n",
            "Epoch 00002: val_loss improved from 3653.81909 to 684.90820, saving model to ./model/02-684.9082.hdf5\n",
            "Epoch 3/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 582.2314 - val_loss: 561.9515\n",
            "\n",
            "Epoch 00003: val_loss improved from 684.90820 to 561.95148, saving model to ./model/03-561.9515.hdf5\n",
            "Epoch 4/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 508.8246 - val_loss: 487.4759\n",
            "\n",
            "Epoch 00004: val_loss improved from 561.95148 to 487.47589, saving model to ./model/04-487.4759.hdf5\n",
            "Epoch 5/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 428.7079 - val_loss: 400.7823\n",
            "\n",
            "Epoch 00005: val_loss improved from 487.47589 to 400.78232, saving model to ./model/05-400.7823.hdf5\n",
            "Epoch 6/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 338.3978 - val_loss: 308.7238\n",
            "\n",
            "Epoch 00006: val_loss improved from 400.78232 to 308.72382, saving model to ./model/06-308.7238.hdf5\n",
            "Epoch 7/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 248.1040 - val_loss: 230.3115\n",
            "\n",
            "Epoch 00007: val_loss improved from 308.72382 to 230.31154, saving model to ./model/07-230.3115.hdf5\n",
            "Epoch 8/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 179.5336 - val_loss: 181.8468\n",
            "\n",
            "Epoch 00008: val_loss improved from 230.31154 to 181.84683, saving model to ./model/08-181.8468.hdf5\n",
            "Epoch 9/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 138.9834 - val_loss: 163.0158\n",
            "\n",
            "Epoch 00009: val_loss improved from 181.84683 to 163.01582, saving model to ./model/09-163.0158.hdf5\n",
            "Epoch 10/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 123.0970 - val_loss: 155.7281\n",
            "\n",
            "Epoch 00010: val_loss improved from 163.01582 to 155.72809, saving model to ./model/10-155.7281.hdf5\n",
            "Epoch 11/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 115.2851 - val_loss: 151.0361\n",
            "\n",
            "Epoch 00011: val_loss improved from 155.72809 to 151.03612, saving model to ./model/11-151.0361.hdf5\n",
            "Epoch 12/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 110.6239 - val_loss: 145.5668\n",
            "\n",
            "Epoch 00012: val_loss improved from 151.03612 to 145.56683, saving model to ./model/12-145.5668.hdf5\n",
            "Epoch 13/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 106.0473 - val_loss: 141.8040\n",
            "\n",
            "Epoch 00013: val_loss improved from 145.56683 to 141.80403, saving model to ./model/13-141.8040.hdf5\n",
            "Epoch 14/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 99.4225 - val_loss: 123.6884\n",
            "\n",
            "Epoch 00014: val_loss improved from 141.80403 to 123.68842, saving model to ./model/14-123.6884.hdf5\n",
            "Epoch 15/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 80.9938 - val_loss: 106.8375\n",
            "\n",
            "Epoch 00015: val_loss improved from 123.68842 to 106.83746, saving model to ./model/15-106.8375.hdf5\n",
            "Epoch 16/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 73.6541 - val_loss: 98.1333\n",
            "\n",
            "Epoch 00016: val_loss improved from 106.83746 to 98.13329, saving model to ./model/16-98.1333.hdf5\n",
            "Epoch 17/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 70.7726 - val_loss: 92.1521\n",
            "\n",
            "Epoch 00017: val_loss improved from 98.13329 to 92.15206, saving model to ./model/17-92.1521.hdf5\n",
            "Epoch 18/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 69.7657 - val_loss: 92.5660\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 92.15206\n",
            "Epoch 19/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 67.8010 - val_loss: 90.7880\n",
            "\n",
            "Epoch 00019: val_loss improved from 92.15206 to 90.78802, saving model to ./model/19-90.7880.hdf5\n",
            "Epoch 20/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 67.8718 - val_loss: 82.8401\n",
            "\n",
            "Epoch 00020: val_loss improved from 90.78802 to 82.84010, saving model to ./model/20-82.8401.hdf5\n",
            "Epoch 21/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 64.7910 - val_loss: 80.5533\n",
            "\n",
            "Epoch 00021: val_loss improved from 82.84010 to 80.55332, saving model to ./model/21-80.5533.hdf5\n",
            "Epoch 22/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 64.2041 - val_loss: 80.6544\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 80.55332\n",
            "Epoch 23/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 67.8066 - val_loss: 77.6381\n",
            "\n",
            "Epoch 00023: val_loss improved from 80.55332 to 77.63811, saving model to ./model/23-77.6381.hdf5\n",
            "Epoch 24/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 62.3047 - val_loss: 77.3940\n",
            "\n",
            "Epoch 00024: val_loss improved from 77.63811 to 77.39396, saving model to ./model/24-77.3940.hdf5\n",
            "Epoch 25/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 62.2905 - val_loss: 73.8579\n",
            "\n",
            "Epoch 00025: val_loss improved from 77.39396 to 73.85793, saving model to ./model/25-73.8579.hdf5\n",
            "Epoch 26/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 61.7205 - val_loss: 72.6196\n",
            "\n",
            "Epoch 00026: val_loss improved from 73.85793 to 72.61961, saving model to ./model/26-72.6196.hdf5\n",
            "Epoch 27/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 61.9613 - val_loss: 72.2753\n",
            "\n",
            "Epoch 00027: val_loss improved from 72.61961 to 72.27531, saving model to ./model/27-72.2753.hdf5\n",
            "Epoch 28/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 60.3489 - val_loss: 71.3226\n",
            "\n",
            "Epoch 00028: val_loss improved from 72.27531 to 71.32259, saving model to ./model/28-71.3226.hdf5\n",
            "Epoch 29/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 60.0331 - val_loss: 70.0235\n",
            "\n",
            "Epoch 00029: val_loss improved from 71.32259 to 70.02348, saving model to ./model/29-70.0235.hdf5\n",
            "Epoch 30/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 59.2628 - val_loss: 69.4668\n",
            "\n",
            "Epoch 00030: val_loss improved from 70.02348 to 69.46676, saving model to ./model/30-69.4668.hdf5\n",
            "Epoch 31/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 58.6504 - val_loss: 69.1077\n",
            "\n",
            "Epoch 00031: val_loss improved from 69.46676 to 69.10770, saving model to ./model/31-69.1077.hdf5\n",
            "Epoch 32/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 58.8534 - val_loss: 68.7700\n",
            "\n",
            "Epoch 00032: val_loss improved from 69.10770 to 68.77003, saving model to ./model/32-68.7700.hdf5\n",
            "Epoch 33/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 59.3119 - val_loss: 68.4114\n",
            "\n",
            "Epoch 00033: val_loss improved from 68.77003 to 68.41141, saving model to ./model/33-68.4114.hdf5\n",
            "Epoch 34/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 58.8807 - val_loss: 67.4663\n",
            "\n",
            "Epoch 00034: val_loss improved from 68.41141 to 67.46632, saving model to ./model/34-67.4663.hdf5\n",
            "Epoch 35/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 58.0675 - val_loss: 67.3259\n",
            "\n",
            "Epoch 00035: val_loss improved from 67.46632 to 67.32594, saving model to ./model/35-67.3259.hdf5\n",
            "Epoch 36/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 58.1890 - val_loss: 66.4974\n",
            "\n",
            "Epoch 00036: val_loss improved from 67.32594 to 66.49741, saving model to ./model/36-66.4974.hdf5\n",
            "Epoch 37/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 58.0277 - val_loss: 65.0791\n",
            "\n",
            "Epoch 00037: val_loss improved from 66.49741 to 65.07907, saving model to ./model/37-65.0791.hdf5\n",
            "Epoch 38/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 57.8483 - val_loss: 64.5019\n",
            "\n",
            "Epoch 00038: val_loss improved from 65.07907 to 64.50190, saving model to ./model/38-64.5019.hdf5\n",
            "Epoch 39/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 57.6147 - val_loss: 63.9164\n",
            "\n",
            "Epoch 00039: val_loss improved from 64.50190 to 63.91639, saving model to ./model/39-63.9164.hdf5\n",
            "Epoch 40/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 56.4151 - val_loss: 63.7250\n",
            "\n",
            "Epoch 00040: val_loss improved from 63.91639 to 63.72496, saving model to ./model/40-63.7250.hdf5\n",
            "Epoch 41/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 56.3574 - val_loss: 63.8281\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 63.72496\n",
            "Epoch 42/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 56.0060 - val_loss: 62.6088\n",
            "\n",
            "Epoch 00042: val_loss improved from 63.72496 to 62.60880, saving model to ./model/42-62.6088.hdf5\n",
            "Epoch 43/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 54.4289 - val_loss: 61.7736\n",
            "\n",
            "Epoch 00043: val_loss improved from 62.60880 to 61.77359, saving model to ./model/43-61.7736.hdf5\n",
            "Epoch 44/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 53.9768 - val_loss: 62.2617\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 61.77359\n",
            "Epoch 45/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 54.7209 - val_loss: 62.3415\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 61.77359\n",
            "Epoch 46/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 51.9488 - val_loss: 60.1530\n",
            "\n",
            "Epoch 00046: val_loss improved from 61.77359 to 60.15304, saving model to ./model/46-60.1530.hdf5\n",
            "Epoch 47/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 51.6957 - val_loss: 59.1232\n",
            "\n",
            "Epoch 00047: val_loss improved from 60.15304 to 59.12321, saving model to ./model/47-59.1232.hdf5\n",
            "Epoch 48/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 51.9826 - val_loss: 58.3466\n",
            "\n",
            "Epoch 00048: val_loss improved from 59.12321 to 58.34665, saving model to ./model/48-58.3466.hdf5\n",
            "Epoch 49/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 51.7209 - val_loss: 57.8927\n",
            "\n",
            "Epoch 00049: val_loss improved from 58.34665 to 57.89275, saving model to ./model/49-57.8927.hdf5\n",
            "Epoch 50/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 50.6318 - val_loss: 57.6620\n",
            "\n",
            "Epoch 00050: val_loss improved from 57.89275 to 57.66202, saving model to ./model/50-57.6620.hdf5\n",
            "Epoch 51/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 49.7443 - val_loss: 57.1039\n",
            "\n",
            "Epoch 00051: val_loss improved from 57.66202 to 57.10393, saving model to ./model/51-57.1039.hdf5\n",
            "Epoch 52/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 49.5030 - val_loss: 56.5042\n",
            "\n",
            "Epoch 00052: val_loss improved from 57.10393 to 56.50423, saving model to ./model/52-56.5042.hdf5\n",
            "Epoch 53/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 48.6788 - val_loss: 56.5756\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 56.50423\n",
            "Epoch 54/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 48.5876 - val_loss: 56.0649\n",
            "\n",
            "Epoch 00054: val_loss improved from 56.50423 to 56.06486, saving model to ./model/54-56.0649.hdf5\n",
            "Epoch 55/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 51.5600 - val_loss: 57.3150\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 56.06486\n",
            "Epoch 56/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 50.2417 - val_loss: 56.9366\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 56.06486\n",
            "Epoch 57/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 50.0856 - val_loss: 54.1750\n",
            "\n",
            "Epoch 00057: val_loss improved from 56.06486 to 54.17499, saving model to ./model/57-54.1750.hdf5\n",
            "Epoch 58/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 46.9344 - val_loss: 55.1633\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 54.17499\n",
            "Epoch 59/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 48.2262 - val_loss: 52.8288\n",
            "\n",
            "Epoch 00059: val_loss improved from 54.17499 to 52.82884, saving model to ./model/59-52.8288.hdf5\n",
            "Epoch 60/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 47.7825 - val_loss: 55.2231\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 52.82884\n",
            "Epoch 61/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 44.7051 - val_loss: 50.7563\n",
            "\n",
            "Epoch 00061: val_loss improved from 52.82884 to 50.75627, saving model to ./model/61-50.7563.hdf5\n",
            "Epoch 62/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 45.0399 - val_loss: 52.3903\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 50.75627\n",
            "Epoch 63/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 44.0822 - val_loss: 49.6764\n",
            "\n",
            "Epoch 00063: val_loss improved from 50.75627 to 49.67636, saving model to ./model/63-49.6764.hdf5\n",
            "Epoch 64/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 43.8251 - val_loss: 50.4641\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 49.67636\n",
            "Epoch 65/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 43.8380 - val_loss: 48.7107\n",
            "\n",
            "Epoch 00065: val_loss improved from 49.67636 to 48.71071, saving model to ./model/65-48.7107.hdf5\n",
            "Epoch 66/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 42.4288 - val_loss: 49.3330\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 48.71071\n",
            "Epoch 67/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 42.6232 - val_loss: 47.6704\n",
            "\n",
            "Epoch 00067: val_loss improved from 48.71071 to 47.67037, saving model to ./model/67-47.6704.hdf5\n",
            "Epoch 68/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 42.3092 - val_loss: 50.8294\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 47.67037\n",
            "Epoch 69/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 41.8185 - val_loss: 46.7822\n",
            "\n",
            "Epoch 00069: val_loss improved from 47.67037 to 46.78224, saving model to ./model/69-46.7822.hdf5\n",
            "Epoch 70/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 40.8907 - val_loss: 48.3274\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 46.78224\n",
            "Epoch 71/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 41.6577 - val_loss: 45.8316\n",
            "\n",
            "Epoch 00071: val_loss improved from 46.78224 to 45.83160, saving model to ./model/71-45.8316.hdf5\n",
            "Epoch 72/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 46.0820 - val_loss: 52.3465\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 45.83160\n",
            "Epoch 73/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 45.1298 - val_loss: 45.8821\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 45.83160\n",
            "Epoch 74/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 39.5967 - val_loss: 45.3706\n",
            "\n",
            "Epoch 00074: val_loss improved from 45.83160 to 45.37057, saving model to ./model/74-45.3706.hdf5\n",
            "Epoch 75/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 40.8118 - val_loss: 44.2258\n",
            "\n",
            "Epoch 00075: val_loss improved from 45.37057 to 44.22579, saving model to ./model/75-44.2258.hdf5\n",
            "Epoch 76/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 39.1989 - val_loss: 43.7256\n",
            "\n",
            "Epoch 00076: val_loss improved from 44.22579 to 43.72564, saving model to ./model/76-43.7256.hdf5\n",
            "Epoch 77/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 38.5959 - val_loss: 45.2687\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 43.72564\n",
            "Epoch 78/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 37.1455 - val_loss: 44.2742\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 43.72564\n",
            "Epoch 79/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 38.1820 - val_loss: 42.8620\n",
            "\n",
            "Epoch 00079: val_loss improved from 43.72564 to 42.86196, saving model to ./model/79-42.8620.hdf5\n",
            "Epoch 80/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 37.4333 - val_loss: 45.7865\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 42.86196\n",
            "Epoch 81/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 37.6682 - val_loss: 43.8813\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 42.86196\n",
            "Epoch 82/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 40.9056 - val_loss: 48.7127\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 42.86196\n",
            "Epoch 83/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 36.4141 - val_loss: 41.3519\n",
            "\n",
            "Epoch 00083: val_loss improved from 42.86196 to 41.35193, saving model to ./model/83-41.3519.hdf5\n",
            "Epoch 84/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 35.9010 - val_loss: 41.3904\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 41.35193\n",
            "Epoch 85/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 35.9270 - val_loss: 41.8773\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 41.35193\n",
            "Epoch 86/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 35.8262 - val_loss: 40.3248\n",
            "\n",
            "Epoch 00086: val_loss improved from 41.35193 to 40.32476, saving model to ./model/86-40.3248.hdf5\n",
            "Epoch 87/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 35.3377 - val_loss: 45.9252\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 40.32476\n",
            "Epoch 88/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 37.5491 - val_loss: 45.8120\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 40.32476\n",
            "Epoch 89/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 37.1668 - val_loss: 40.0015\n",
            "\n",
            "Epoch 00089: val_loss improved from 40.32476 to 40.00152, saving model to ./model/89-40.0015.hdf5\n",
            "Epoch 90/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 34.6786 - val_loss: 39.5834\n",
            "\n",
            "Epoch 00090: val_loss improved from 40.00152 to 39.58336, saving model to ./model/90-39.5834.hdf5\n",
            "Epoch 91/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 33.8987 - val_loss: 40.1029\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 39.58336\n",
            "Epoch 92/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 33.8196 - val_loss: 39.5873\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 39.58336\n",
            "Epoch 93/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 33.7480 - val_loss: 38.7297\n",
            "\n",
            "Epoch 00093: val_loss improved from 39.58336 to 38.72972, saving model to ./model/93-38.7297.hdf5\n",
            "Epoch 94/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 33.4468 - val_loss: 38.0800\n",
            "\n",
            "Epoch 00094: val_loss improved from 38.72972 to 38.08004, saving model to ./model/94-38.0800.hdf5\n",
            "Epoch 95/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 33.5824 - val_loss: 39.9166\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 38.08004\n",
            "Epoch 96/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 32.5472 - val_loss: 38.5388\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 38.08004\n",
            "Epoch 97/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 33.0082 - val_loss: 37.3970\n",
            "\n",
            "Epoch 00097: val_loss improved from 38.08004 to 37.39701, saving model to ./model/97-37.3970.hdf5\n",
            "Epoch 98/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 32.0899 - val_loss: 37.0467\n",
            "\n",
            "Epoch 00098: val_loss improved from 37.39701 to 37.04675, saving model to ./model/98-37.0467.hdf5\n",
            "Epoch 99/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 34.2239 - val_loss: 37.9869\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 37.04675\n",
            "Epoch 100/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 31.2697 - val_loss: 36.4188\n",
            "\n",
            "Epoch 00100: val_loss improved from 37.04675 to 36.41882, saving model to ./model/100-36.4188.hdf5\n",
            "Epoch 101/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 30.8790 - val_loss: 39.4123\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 36.41882\n",
            "Epoch 102/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 32.6375 - val_loss: 36.8885\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 36.41882\n",
            "Epoch 103/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 31.2408 - val_loss: 35.8099\n",
            "\n",
            "Epoch 00103: val_loss improved from 36.41882 to 35.80987, saving model to ./model/103-35.8099.hdf5\n",
            "Epoch 104/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 31.1531 - val_loss: 35.9908\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 35.80987\n",
            "Epoch 105/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 30.2930 - val_loss: 38.0788\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 35.80987\n",
            "Epoch 106/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 32.8074 - val_loss: 36.9957\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 35.80987\n",
            "Epoch 107/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 29.9935 - val_loss: 34.8168\n",
            "\n",
            "Epoch 00107: val_loss improved from 35.80987 to 34.81680, saving model to ./model/107-34.8168.hdf5\n",
            "Epoch 108/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 29.9612 - val_loss: 35.1242\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 34.81680\n",
            "Epoch 109/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 29.4047 - val_loss: 36.9036\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 34.81680\n",
            "Epoch 110/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 31.3140 - val_loss: 34.1522\n",
            "\n",
            "Epoch 00110: val_loss improved from 34.81680 to 34.15216, saving model to ./model/110-34.1522.hdf5\n",
            "Epoch 111/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 31.6260 - val_loss: 33.6404\n",
            "\n",
            "Epoch 00111: val_loss improved from 34.15216 to 33.64042, saving model to ./model/111-33.6404.hdf5\n",
            "Epoch 112/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 29.4897 - val_loss: 34.3329\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 33.64042\n",
            "Epoch 113/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 29.2033 - val_loss: 33.3241\n",
            "\n",
            "Epoch 00113: val_loss improved from 33.64042 to 33.32411, saving model to ./model/113-33.3241.hdf5\n",
            "Epoch 114/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 28.3031 - val_loss: 35.3489\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 33.32411\n",
            "Epoch 115/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 27.8027 - val_loss: 32.6486\n",
            "\n",
            "Epoch 00115: val_loss improved from 33.32411 to 32.64857, saving model to ./model/115-32.6486.hdf5\n",
            "Epoch 116/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 27.7093 - val_loss: 32.6781\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 32.64857\n",
            "Epoch 117/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 28.5294 - val_loss: 34.0569\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 32.64857\n",
            "Epoch 118/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 29.2027 - val_loss: 31.7770\n",
            "\n",
            "Epoch 00118: val_loss improved from 32.64857 to 31.77696, saving model to ./model/118-31.7770.hdf5\n",
            "Epoch 119/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 27.8330 - val_loss: 31.5765\n",
            "\n",
            "Epoch 00119: val_loss improved from 31.77696 to 31.57650, saving model to ./model/119-31.5765.hdf5\n",
            "Epoch 120/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 27.5467 - val_loss: 31.3665\n",
            "\n",
            "Epoch 00120: val_loss improved from 31.57650 to 31.36645, saving model to ./model/120-31.3665.hdf5\n",
            "Epoch 121/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 27.1527 - val_loss: 31.1558\n",
            "\n",
            "Epoch 00121: val_loss improved from 31.36645 to 31.15576, saving model to ./model/121-31.1558.hdf5\n",
            "Epoch 122/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 27.0421 - val_loss: 33.7766\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 31.15576\n",
            "Epoch 123/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 27.5397 - val_loss: 33.5673\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 31.15576\n",
            "Epoch 124/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 27.0855 - val_loss: 30.5743\n",
            "\n",
            "Epoch 00124: val_loss improved from 31.15576 to 30.57430, saving model to ./model/124-30.5743.hdf5\n",
            "Epoch 125/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 27.5513 - val_loss: 30.3809\n",
            "\n",
            "Epoch 00125: val_loss improved from 30.57430 to 30.38093, saving model to ./model/125-30.3809.hdf5\n",
            "Epoch 126/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 27.1863 - val_loss: 31.1643\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 30.38093\n",
            "Epoch 127/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 27.2386 - val_loss: 39.0879\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 30.38093\n",
            "Epoch 128/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 28.5259 - val_loss: 31.4147\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 30.38093\n",
            "Epoch 129/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 26.8397 - val_loss: 32.4641\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 30.38093\n",
            "Epoch 130/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 27.5364 - val_loss: 29.5978\n",
            "\n",
            "Epoch 00130: val_loss improved from 30.38093 to 29.59783, saving model to ./model/130-29.5978.hdf5\n",
            "Epoch 131/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 26.3981 - val_loss: 29.3708\n",
            "\n",
            "Epoch 00131: val_loss improved from 29.59783 to 29.37076, saving model to ./model/131-29.3708.hdf5\n",
            "Epoch 132/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 27.2207 - val_loss: 31.3041\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 29.37076\n",
            "Epoch 133/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 26.7343 - val_loss: 28.8416\n",
            "\n",
            "Epoch 00133: val_loss improved from 29.37076 to 28.84158, saving model to ./model/133-28.8416.hdf5\n",
            "Epoch 134/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 29.2870 - val_loss: 31.2977\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 28.84158\n",
            "Epoch 135/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 25.0440 - val_loss: 29.1540\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 28.84158\n",
            "Epoch 136/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 25.0291 - val_loss: 28.8387\n",
            "\n",
            "Epoch 00136: val_loss improved from 28.84158 to 28.83872, saving model to ./model/136-28.8387.hdf5\n",
            "Epoch 137/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 25.6070 - val_loss: 28.4164\n",
            "\n",
            "Epoch 00137: val_loss improved from 28.83872 to 28.41642, saving model to ./model/137-28.4164.hdf5\n",
            "Epoch 138/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 24.9613 - val_loss: 28.5897\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 28.41642\n",
            "Epoch 139/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 25.7362 - val_loss: 28.0659\n",
            "\n",
            "Epoch 00139: val_loss improved from 28.41642 to 28.06590, saving model to ./model/139-28.0659.hdf5\n",
            "Epoch 140/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 25.7426 - val_loss: 28.5654\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 28.06590\n",
            "Epoch 141/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 25.8476 - val_loss: 28.4538\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 28.06590\n",
            "Epoch 142/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 27.3802 - val_loss: 28.0400\n",
            "\n",
            "Epoch 00142: val_loss improved from 28.06590 to 28.03999, saving model to ./model/142-28.0400.hdf5\n",
            "Epoch 143/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 24.6844 - val_loss: 27.6902\n",
            "\n",
            "Epoch 00143: val_loss improved from 28.03999 to 27.69017, saving model to ./model/143-27.6902.hdf5\n",
            "Epoch 144/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 24.5215 - val_loss: 28.1786\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 27.69017\n",
            "Epoch 145/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 24.1022 - val_loss: 28.8150\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 27.69017\n",
            "Epoch 146/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 24.2015 - val_loss: 27.3447\n",
            "\n",
            "Epoch 00146: val_loss improved from 27.69017 to 27.34471, saving model to ./model/146-27.3447.hdf5\n",
            "Epoch 147/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 24.4027 - val_loss: 27.1083\n",
            "\n",
            "Epoch 00147: val_loss improved from 27.34471 to 27.10833, saving model to ./model/147-27.1083.hdf5\n",
            "Epoch 148/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 24.4234 - val_loss: 26.9328\n",
            "\n",
            "Epoch 00148: val_loss improved from 27.10833 to 26.93279, saving model to ./model/148-26.9328.hdf5\n",
            "Epoch 149/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 24.3526 - val_loss: 26.8778\n",
            "\n",
            "Epoch 00149: val_loss improved from 26.93279 to 26.87780, saving model to ./model/149-26.8778.hdf5\n",
            "Epoch 150/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 25.9973 - val_loss: 35.7505\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 26.87780\n",
            "Epoch 151/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 27.2076 - val_loss: 27.2550\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 26.87780\n",
            "Epoch 152/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 24.3417 - val_loss: 26.8968\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 26.87780\n",
            "Epoch 153/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 24.3243 - val_loss: 27.8293\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 26.87780\n",
            "Epoch 154/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 23.7886 - val_loss: 26.8862\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 26.87780\n",
            "Epoch 155/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 23.9117 - val_loss: 26.3141\n",
            "\n",
            "Epoch 00155: val_loss improved from 26.87780 to 26.31414, saving model to ./model/155-26.3141.hdf5\n",
            "Epoch 156/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 23.4874 - val_loss: 26.5129\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 26.31414\n",
            "Epoch 157/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 22.2580 - val_loss: 27.6393\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 26.31414\n",
            "Epoch 158/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 24.3299 - val_loss: 25.8456\n",
            "\n",
            "Epoch 00158: val_loss improved from 26.31414 to 25.84556, saving model to ./model/158-25.8456.hdf5\n",
            "Epoch 159/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 23.6423 - val_loss: 28.5197\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 25.84556\n",
            "Epoch 160/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 24.8157 - val_loss: 32.6449\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 25.84556\n",
            "Epoch 161/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 24.3387 - val_loss: 26.0572\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 25.84556\n",
            "Epoch 162/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 23.7141 - val_loss: 25.4256\n",
            "\n",
            "Epoch 00162: val_loss improved from 25.84556 to 25.42561, saving model to ./model/162-25.4256.hdf5\n",
            "Epoch 163/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 23.6645 - val_loss: 25.5812\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 25.42561\n",
            "Epoch 164/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 22.8991 - val_loss: 25.4107\n",
            "\n",
            "Epoch 00164: val_loss improved from 25.42561 to 25.41074, saving model to ./model/164-25.4107.hdf5\n",
            "Epoch 165/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 22.4769 - val_loss: 25.3645\n",
            "\n",
            "Epoch 00165: val_loss improved from 25.41074 to 25.36452, saving model to ./model/165-25.3645.hdf5\n",
            "Epoch 166/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 23.1597 - val_loss: 25.5825\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 25.36452\n",
            "Epoch 167/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 22.4671 - val_loss: 25.2054\n",
            "\n",
            "Epoch 00167: val_loss improved from 25.36452 to 25.20541, saving model to ./model/167-25.2054.hdf5\n",
            "Epoch 168/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 23.9653 - val_loss: 30.5510\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 25.20541\n",
            "Epoch 169/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 27.4858 - val_loss: 25.5287\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 25.20541\n",
            "Epoch 170/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 23.0210 - val_loss: 24.9795\n",
            "\n",
            "Epoch 00170: val_loss improved from 25.20541 to 24.97955, saving model to ./model/170-24.9795.hdf5\n",
            "Epoch 171/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 22.3482 - val_loss: 33.2210\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 24.97955\n",
            "Epoch 172/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 23.2952 - val_loss: 25.1484\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 24.97955\n",
            "Epoch 173/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 23.9752 - val_loss: 24.1962\n",
            "\n",
            "Epoch 00173: val_loss improved from 24.97955 to 24.19622, saving model to ./model/173-24.1962.hdf5\n",
            "Epoch 174/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 21.9624 - val_loss: 25.1876\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 24.19622\n",
            "Epoch 175/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 22.8232 - val_loss: 24.1283\n",
            "\n",
            "Epoch 00175: val_loss improved from 24.19622 to 24.12833, saving model to ./model/175-24.1283.hdf5\n",
            "Epoch 176/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 22.7613 - val_loss: 23.9749\n",
            "\n",
            "Epoch 00176: val_loss improved from 24.12833 to 23.97485, saving model to ./model/176-23.9749.hdf5\n",
            "Epoch 177/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 22.0501 - val_loss: 24.3435\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 23.97485\n",
            "Epoch 178/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 21.8415 - val_loss: 23.9146\n",
            "\n",
            "Epoch 00178: val_loss improved from 23.97485 to 23.91463, saving model to ./model/178-23.9146.hdf5\n",
            "Epoch 179/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 21.6575 - val_loss: 23.9370\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 23.91463\n",
            "Epoch 180/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 21.8931 - val_loss: 25.8517\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 23.91463\n",
            "Epoch 181/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 23.5455 - val_loss: 25.0135\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 23.91463\n",
            "Epoch 182/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 22.8699 - val_loss: 27.2834\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 23.91463\n",
            "Epoch 183/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 23.1005 - val_loss: 23.7795\n",
            "\n",
            "Epoch 00183: val_loss improved from 23.91463 to 23.77950, saving model to ./model/183-23.7795.hdf5\n",
            "Epoch 184/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 24.2799 - val_loss: 33.0780\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 23.77950\n",
            "Epoch 185/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 22.8104 - val_loss: 23.4618\n",
            "\n",
            "Epoch 00185: val_loss improved from 23.77950 to 23.46180, saving model to ./model/185-23.4618.hdf5\n",
            "Epoch 186/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 21.5860 - val_loss: 28.0673\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 23.46180\n",
            "Epoch 187/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 20.8726 - val_loss: 23.3658\n",
            "\n",
            "Epoch 00187: val_loss improved from 23.46180 to 23.36580, saving model to ./model/187-23.3658.hdf5\n",
            "Epoch 188/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 21.1306 - val_loss: 23.4531\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 23.36580\n",
            "Epoch 189/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 22.4906 - val_loss: 23.9497\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 23.36580\n",
            "Epoch 190/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 21.4558 - val_loss: 23.2691\n",
            "\n",
            "Epoch 00190: val_loss improved from 23.36580 to 23.26915, saving model to ./model/190-23.2691.hdf5\n",
            "Epoch 191/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 23.0195 - val_loss: 25.0115\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 23.26915\n",
            "Epoch 192/2000\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24.0603 - val_loss: 23.9959\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 23.26915\n",
            "Epoch 193/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 22.7602 - val_loss: 23.4809\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 23.26915\n",
            "Epoch 194/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 22.6178 - val_loss: 23.0002\n",
            "\n",
            "Epoch 00194: val_loss improved from 23.26915 to 23.00015, saving model to ./model/194-23.0002.hdf5\n",
            "Epoch 195/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 20.7690 - val_loss: 23.4829\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 23.00015\n",
            "Epoch 196/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 21.2601 - val_loss: 26.2383\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 23.00015\n",
            "Epoch 197/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 21.3538 - val_loss: 22.8980\n",
            "\n",
            "Epoch 00197: val_loss improved from 23.00015 to 22.89796, saving model to ./model/197-22.8980.hdf5\n",
            "Epoch 198/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 20.5294 - val_loss: 23.4724\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 22.89796\n",
            "Epoch 199/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 24.4160 - val_loss: 23.9510\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 22.89796\n",
            "Epoch 200/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 21.5408 - val_loss: 23.1776\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 22.89796\n",
            "Epoch 201/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 20.1668 - val_loss: 22.4127\n",
            "\n",
            "Epoch 00201: val_loss improved from 22.89796 to 22.41269, saving model to ./model/201-22.4127.hdf5\n",
            "Epoch 202/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 22.0730 - val_loss: 28.1246\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 22.41269\n",
            "Epoch 203/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 21.0997 - val_loss: 22.6881\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 22.41269\n",
            "Epoch 204/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 21.2951 - val_loss: 22.4322\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 22.41269\n",
            "Epoch 205/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 19.8655 - val_loss: 22.1156\n",
            "\n",
            "Epoch 00205: val_loss improved from 22.41269 to 22.11563, saving model to ./model/205-22.1156.hdf5\n",
            "Epoch 206/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 21.7550 - val_loss: 24.8956\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 22.11563\n",
            "Epoch 207/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 20.3443 - val_loss: 22.4171\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 22.11563\n",
            "Epoch 208/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 20.0618 - val_loss: 21.0979\n",
            "\n",
            "Epoch 00208: val_loss improved from 22.11563 to 21.09788, saving model to ./model/208-21.0979.hdf5\n",
            "Epoch 209/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 19.3136 - val_loss: 21.7839\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 21.09788\n",
            "Epoch 210/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 19.1370 - val_loss: 22.4671\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 21.09788\n",
            "Epoch 211/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 20.3526 - val_loss: 23.0118\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 21.09788\n",
            "Epoch 212/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 19.6082 - val_loss: 20.9922\n",
            "\n",
            "Epoch 00212: val_loss improved from 21.09788 to 20.99224, saving model to ./model/212-20.9922.hdf5\n",
            "Epoch 213/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 19.8309 - val_loss: 21.4581\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 20.99224\n",
            "Epoch 214/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 19.2403 - val_loss: 22.7214\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 20.99224\n",
            "Epoch 215/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 19.1917 - val_loss: 22.4579\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 20.99224\n",
            "Epoch 216/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 22.5751 - val_loss: 23.0832\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 20.99224\n",
            "Epoch 217/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.8811 - val_loss: 22.1637\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 20.99224\n",
            "Epoch 218/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.7020 - val_loss: 21.3390\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 20.99224\n",
            "Epoch 219/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 19.8185 - val_loss: 21.2379\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 20.99224\n",
            "Epoch 220/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 20.4077 - val_loss: 20.4280\n",
            "\n",
            "Epoch 00220: val_loss improved from 20.99224 to 20.42798, saving model to ./model/220-20.4280.hdf5\n",
            "Epoch 221/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 19.7895 - val_loss: 21.4325\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 20.42798\n",
            "Epoch 222/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 19.1212 - val_loss: 20.3414\n",
            "\n",
            "Epoch 00222: val_loss improved from 20.42798 to 20.34137, saving model to ./model/222-20.3414.hdf5\n",
            "Epoch 223/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 19.3786 - val_loss: 21.2053\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 20.34137\n",
            "Epoch 224/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 20.2109 - val_loss: 27.7274\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 20.34137\n",
            "Epoch 225/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 23.9256 - val_loss: 22.3268\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 20.34137\n",
            "Epoch 226/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 19.2754 - val_loss: 21.9256\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 20.34137\n",
            "Epoch 227/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 19.5536 - val_loss: 22.1465\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 20.34137\n",
            "Epoch 228/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.7551 - val_loss: 21.3190\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 20.34137\n",
            "Epoch 229/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.9875 - val_loss: 22.7606\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 20.34137\n",
            "Epoch 230/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.7184 - val_loss: 28.5157\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 20.34137\n",
            "Epoch 231/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 23.4647 - val_loss: 21.5003\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 20.34137\n",
            "Epoch 232/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 18.1333 - val_loss: 22.6395\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 20.34137\n",
            "Epoch 233/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.9929 - val_loss: 20.3740\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 20.34137\n",
            "Epoch 234/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.0945 - val_loss: 21.2371\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 20.34137\n",
            "Epoch 235/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 19.5406 - val_loss: 23.4835\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 20.34137\n",
            "Epoch 236/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 19.1755 - val_loss: 20.0152\n",
            "\n",
            "Epoch 00236: val_loss improved from 20.34137 to 20.01517, saving model to ./model/236-20.0152.hdf5\n",
            "Epoch 237/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.0627 - val_loss: 19.7289\n",
            "\n",
            "Epoch 00237: val_loss improved from 20.01517 to 19.72892, saving model to ./model/237-19.7289.hdf5\n",
            "Epoch 238/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.3556 - val_loss: 19.6202\n",
            "\n",
            "Epoch 00238: val_loss improved from 19.72892 to 19.62025, saving model to ./model/238-19.6202.hdf5\n",
            "Epoch 239/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.4227 - val_loss: 21.5942\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 19.62025\n",
            "Epoch 240/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 19.5637 - val_loss: 20.6041\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 19.62025\n",
            "Epoch 241/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 18.2581 - val_loss: 19.5810\n",
            "\n",
            "Epoch 00241: val_loss improved from 19.62025 to 19.58102, saving model to ./model/241-19.5810.hdf5\n",
            "Epoch 242/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.1462 - val_loss: 19.4485\n",
            "\n",
            "Epoch 00242: val_loss improved from 19.58102 to 19.44851, saving model to ./model/242-19.4485.hdf5\n",
            "Epoch 243/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.2960 - val_loss: 19.9747\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 19.44851\n",
            "Epoch 244/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 20.3734 - val_loss: 20.1960\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 19.44851\n",
            "Epoch 245/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.2048 - val_loss: 19.4866\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 19.44851\n",
            "Epoch 246/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.0047 - val_loss: 20.2861\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 19.44851\n",
            "Epoch 247/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 17.3749 - val_loss: 20.3957\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 19.44851\n",
            "Epoch 248/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 17.4676 - val_loss: 19.7704\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 19.44851\n",
            "Epoch 249/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.7928 - val_loss: 18.9812\n",
            "\n",
            "Epoch 00249: val_loss improved from 19.44851 to 18.98120, saving model to ./model/249-18.9812.hdf5\n",
            "Epoch 250/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.9785 - val_loss: 19.0508\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 18.98120\n",
            "Epoch 251/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 19.1553 - val_loss: 24.3233\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 18.98120\n",
            "Epoch 252/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 19.5156 - val_loss: 19.4530\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 18.98120\n",
            "Epoch 253/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.8922 - val_loss: 20.5224\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 18.98120\n",
            "Epoch 254/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.1211 - val_loss: 19.3560\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 18.98120\n",
            "Epoch 255/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.5517 - val_loss: 20.6362\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 18.98120\n",
            "Epoch 256/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.7577 - val_loss: 22.0277\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 18.98120\n",
            "Epoch 257/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 20.6534 - val_loss: 19.6180\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 18.98120\n",
            "Epoch 258/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.1954 - val_loss: 19.0989\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 18.98120\n",
            "Epoch 259/2000\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 17.3927 - val_loss: 18.8848\n",
            "\n",
            "Epoch 00259: val_loss improved from 18.98120 to 18.88476, saving model to ./model/259-18.8848.hdf5\n",
            "Epoch 260/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.4538 - val_loss: 18.6717\n",
            "\n",
            "Epoch 00260: val_loss improved from 18.88476 to 18.67169, saving model to ./model/260-18.6717.hdf5\n",
            "Epoch 261/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.7432 - val_loss: 18.7043\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 18.67169\n",
            "Epoch 262/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.0952 - val_loss: 18.6884\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 18.67169\n",
            "Epoch 263/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.8820 - val_loss: 18.8428\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 18.67169\n",
            "Epoch 264/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.0148 - val_loss: 24.3645\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 18.67169\n",
            "Epoch 265/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 18.4810 - val_loss: 18.5948\n",
            "\n",
            "Epoch 00265: val_loss improved from 18.67169 to 18.59481, saving model to ./model/265-18.5948.hdf5\n",
            "Epoch 266/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.5627 - val_loss: 18.4129\n",
            "\n",
            "Epoch 00266: val_loss improved from 18.59481 to 18.41289, saving model to ./model/266-18.4129.hdf5\n",
            "Epoch 267/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 17.1153 - val_loss: 21.4668\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 18.41289\n",
            "Epoch 268/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.0153 - val_loss: 24.1455\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 18.41289\n",
            "Epoch 269/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 19.5104 - val_loss: 32.1944\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 18.41289\n",
            "Epoch 270/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 20.4530 - val_loss: 23.0404\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 18.41289\n",
            "Epoch 271/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.4020 - val_loss: 19.4765\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 18.41289\n",
            "Epoch 272/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.5497 - val_loss: 24.7353\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 18.41289\n",
            "Epoch 273/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.2908 - val_loss: 19.4197\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 18.41289\n",
            "Epoch 274/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 19.2843 - val_loss: 18.6547\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 18.41289\n",
            "Epoch 275/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.9287 - val_loss: 19.0595\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 18.41289\n",
            "Epoch 276/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.9349 - val_loss: 18.5657\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 18.41289\n",
            "Epoch 277/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.6940 - val_loss: 18.4880\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 18.41289\n",
            "Epoch 278/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 18.3550 - val_loss: 19.4388\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 18.41289\n",
            "Epoch 279/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.8652 - val_loss: 22.7389\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 18.41289\n",
            "Epoch 280/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.8718 - val_loss: 19.0044\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 18.41289\n",
            "Epoch 281/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.5032 - val_loss: 18.8813\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 18.41289\n",
            "Epoch 282/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 16.8233 - val_loss: 19.2724\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 18.41289\n",
            "Epoch 283/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 16.8837 - val_loss: 21.5586\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 18.41289\n",
            "Epoch 284/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.3016 - val_loss: 18.6714\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 18.41289\n",
            "Epoch 285/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.3133 - val_loss: 20.3025\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 18.41289\n",
            "Epoch 286/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 17.5481 - val_loss: 20.6881\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 18.41289\n",
            "Epoch 287/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 18.4066 - val_loss: 19.2905\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 18.41289\n",
            "Epoch 288/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 20.1581 - val_loss: 24.5873\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 18.41289\n",
            "Epoch 289/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.8931 - val_loss: 20.3216\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 18.41289\n",
            "Epoch 290/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.3928 - val_loss: 18.5171\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 18.41289\n",
            "Epoch 291/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.3891 - val_loss: 18.1116\n",
            "\n",
            "Epoch 00291: val_loss improved from 18.41289 to 18.11157, saving model to ./model/291-18.1116.hdf5\n",
            "Epoch 292/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.6446 - val_loss: 20.6780\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 18.11157\n",
            "Epoch 293/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.1345 - val_loss: 19.2326\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 18.11157\n",
            "Epoch 294/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.3280 - val_loss: 18.1461\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 18.11157\n",
            "Epoch 295/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.6899 - val_loss: 24.4560\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 18.11157\n",
            "Epoch 296/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 20.5262 - val_loss: 22.6820\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 18.11157\n",
            "Epoch 297/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.6191 - val_loss: 19.5746\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 18.11157\n",
            "Epoch 298/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 17.1672 - val_loss: 18.3932\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 18.11157\n",
            "Epoch 299/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.0113 - val_loss: 18.8401\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 18.11157\n",
            "Epoch 300/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.8956 - val_loss: 24.9760\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 18.11157\n",
            "Epoch 301/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 19.5638 - val_loss: 19.0685\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 18.11157\n",
            "Epoch 302/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 19.1861 - val_loss: 25.2903\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 18.11157\n",
            "Epoch 303/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 21.2211 - val_loss: 18.7317\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 18.11157\n",
            "Epoch 304/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 16.5455 - val_loss: 19.2020\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 18.11157\n",
            "Epoch 305/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.5759 - val_loss: 18.2638\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 18.11157\n",
            "Epoch 306/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.1841 - val_loss: 17.8002\n",
            "\n",
            "Epoch 00306: val_loss improved from 18.11157 to 17.80022, saving model to ./model/306-17.8002.hdf5\n",
            "Epoch 307/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.9699 - val_loss: 18.1988\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 17.80022\n",
            "Epoch 308/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 21.5557 - val_loss: 28.0699\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 17.80022\n",
            "Epoch 309/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 18.3183 - val_loss: 19.5353\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 17.80022\n",
            "Epoch 310/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.9123 - val_loss: 18.1914\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 17.80022\n",
            "Epoch 311/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.9389 - val_loss: 17.8311\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 17.80022\n",
            "Epoch 312/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 19.2195 - val_loss: 24.3008\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 17.80022\n",
            "Epoch 313/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.0777 - val_loss: 17.7935\n",
            "\n",
            "Epoch 00313: val_loss improved from 17.80022 to 17.79350, saving model to ./model/313-17.7935.hdf5\n",
            "Epoch 314/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.3310 - val_loss: 18.4245\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 17.79350\n",
            "Epoch 315/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.9497 - val_loss: 30.1939\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 17.79350\n",
            "Epoch 316/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 19.7544 - val_loss: 17.6991\n",
            "\n",
            "Epoch 00316: val_loss improved from 17.79350 to 17.69907, saving model to ./model/316-17.6991.hdf5\n",
            "Epoch 317/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.7286 - val_loss: 19.7032\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 17.69907\n",
            "Epoch 318/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.7143 - val_loss: 21.2097\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 17.69907\n",
            "Epoch 319/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.6724 - val_loss: 21.0750\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 17.69907\n",
            "Epoch 320/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.1428 - val_loss: 18.4952\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 17.69907\n",
            "Epoch 321/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.0452 - val_loss: 20.5875\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 17.69907\n",
            "Epoch 322/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.3206 - val_loss: 18.7268\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 17.69907\n",
            "Epoch 323/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.1307 - val_loss: 18.6955\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 17.69907\n",
            "Epoch 324/2000\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 15.8144 - val_loss: 19.3536\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 17.69907\n",
            "Epoch 325/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.0237 - val_loss: 19.0234\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 17.69907\n",
            "Epoch 326/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.6816 - val_loss: 18.6382\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 17.69907\n",
            "Epoch 327/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 15.6775 - val_loss: 20.8785\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 17.69907\n",
            "Epoch 328/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.9473 - val_loss: 19.6640\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 17.69907\n",
            "Epoch 329/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 16.0167 - val_loss: 20.6958\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 17.69907\n",
            "Epoch 330/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.6118 - val_loss: 18.3672\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 17.69907\n",
            "Epoch 331/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 19.8901 - val_loss: 19.9859\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 17.69907\n",
            "Epoch 332/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.3175 - val_loss: 18.7714\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 17.69907\n",
            "Epoch 333/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.6439 - val_loss: 21.5282\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 17.69907\n",
            "Epoch 334/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.3334 - val_loss: 19.3570\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 17.69907\n",
            "Epoch 335/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.5242 - val_loss: 22.0760\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 17.69907\n",
            "Epoch 336/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.0136 - val_loss: 38.7476\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 17.69907\n",
            "Epoch 337/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.0383 - val_loss: 18.9623\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 17.69907\n",
            "Epoch 338/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 15.8652 - val_loss: 18.3671\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 17.69907\n",
            "Epoch 339/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.7940 - val_loss: 19.0535\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 17.69907\n",
            "Epoch 340/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.8649 - val_loss: 17.7534\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 17.69907\n",
            "Epoch 341/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.6180 - val_loss: 18.0689\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 17.69907\n",
            "Epoch 342/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 16.0925 - val_loss: 17.9378\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 17.69907\n",
            "Epoch 343/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.3029 - val_loss: 28.8337\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 17.69907\n",
            "Epoch 344/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 21.3684 - val_loss: 18.8035\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 17.69907\n",
            "Epoch 345/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.7260 - val_loss: 18.2269\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 17.69907\n",
            "Epoch 346/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.8756 - val_loss: 19.1312\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 17.69907\n",
            "Epoch 347/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.7068 - val_loss: 18.9185\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 17.69907\n",
            "Epoch 348/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 16.5216 - val_loss: 19.6364\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 17.69907\n",
            "Epoch 349/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 17.4398 - val_loss: 20.6770\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 17.69907\n",
            "Epoch 350/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.8573 - val_loss: 18.0680\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 17.69907\n",
            "Epoch 351/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.3371 - val_loss: 18.8309\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 17.69907\n",
            "Epoch 352/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.8723 - val_loss: 20.4858\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 17.69907\n",
            "Epoch 353/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.9691 - val_loss: 20.1335\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 17.69907\n",
            "Epoch 354/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.6051 - val_loss: 18.2212\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 17.69907\n",
            "Epoch 355/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.8864 - val_loss: 17.3751\n",
            "\n",
            "Epoch 00355: val_loss improved from 17.69907 to 17.37512, saving model to ./model/355-17.3751.hdf5\n",
            "Epoch 356/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.7315 - val_loss: 18.0528\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 17.37512\n",
            "Epoch 357/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.8294 - val_loss: 17.6565\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 17.37512\n",
            "Epoch 358/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 16.9059 - val_loss: 17.6961\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 17.37512\n",
            "Epoch 359/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.9943 - val_loss: 17.6592\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 17.37512\n",
            "Epoch 360/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.2350 - val_loss: 17.3156\n",
            "\n",
            "Epoch 00360: val_loss improved from 17.37512 to 17.31557, saving model to ./model/360-17.3156.hdf5\n",
            "Epoch 361/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.4456 - val_loss: 17.8370\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 17.31557\n",
            "Epoch 362/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 15.0916 - val_loss: 24.0632\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 17.31557\n",
            "Epoch 363/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 19.8011 - val_loss: 18.7757\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 17.31557\n",
            "Epoch 364/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.7358 - val_loss: 19.2219\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 17.31557\n",
            "Epoch 365/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 16.1478 - val_loss: 25.3647\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 17.31557\n",
            "Epoch 366/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.9068 - val_loss: 22.0487\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 17.31557\n",
            "Epoch 367/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.3569 - val_loss: 18.1893\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 17.31557\n",
            "Epoch 368/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.1422 - val_loss: 19.0149\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 17.31557\n",
            "Epoch 369/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.0024 - val_loss: 18.5645\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 17.31557\n",
            "Epoch 370/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.2298 - val_loss: 23.7310\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 17.31557\n",
            "Epoch 371/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 17.4478 - val_loss: 20.0484\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 17.31557\n",
            "Epoch 372/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.4271 - val_loss: 20.7959\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 17.31557\n",
            "Epoch 373/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.7720 - val_loss: 21.1632\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 17.31557\n",
            "Epoch 374/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.9265 - val_loss: 19.1612\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 17.31557\n",
            "Epoch 375/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.7311 - val_loss: 24.1636\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 17.31557\n",
            "Epoch 376/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.7041 - val_loss: 18.6653\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 17.31557\n",
            "Epoch 377/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.0320 - val_loss: 20.0865\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 17.31557\n",
            "Epoch 378/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.6807 - val_loss: 19.1857\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 17.31557\n",
            "Epoch 379/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.7810 - val_loss: 18.1984\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 17.31557\n",
            "Epoch 380/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.2121 - val_loss: 20.9381\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 17.31557\n",
            "Epoch 381/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.5201 - val_loss: 18.3894\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 17.31557\n",
            "Epoch 382/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.9594 - val_loss: 18.5131\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 17.31557\n",
            "Epoch 383/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.8778 - val_loss: 20.8350\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 17.31557\n",
            "Epoch 384/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 16.7058 - val_loss: 18.2269\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 17.31557\n",
            "Epoch 385/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 14.8863 - val_loss: 17.9609\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 17.31557\n",
            "Epoch 386/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.6885 - val_loss: 18.5907\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 17.31557\n",
            "Epoch 387/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 16.7886 - val_loss: 19.0259\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 17.31557\n",
            "Epoch 388/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 21.3001 - val_loss: 27.8099\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 17.31557\n",
            "Epoch 389/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 18.0397 - val_loss: 18.7820\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 17.31557\n",
            "Epoch 390/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 16.1241 - val_loss: 17.8309\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 17.31557\n",
            "Epoch 391/2000\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 15.6672 - val_loss: 20.0062\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 17.31557\n",
            "Epoch 392/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.5264 - val_loss: 18.5973\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 17.31557\n",
            "Epoch 393/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.5561 - val_loss: 20.2532\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 17.31557\n",
            "Epoch 394/2000\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 14.7661 - val_loss: 17.8184\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 17.31557\n",
            "Epoch 395/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 15.2544 - val_loss: 18.8049\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 17.31557\n",
            "Epoch 396/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.4394 - val_loss: 18.3607\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 17.31557\n",
            "Epoch 397/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 17.4127 - val_loss: 18.7441\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 17.31557\n",
            "Epoch 398/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 15.5061 - val_loss: 18.3571\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 17.31557\n",
            "Epoch 399/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 16.7624 - val_loss: 18.0995\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 17.31557\n",
            "Epoch 400/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.5469 - val_loss: 18.9832\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 17.31557\n",
            "Epoch 401/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.1730 - val_loss: 19.2052\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 17.31557\n",
            "Epoch 402/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.6893 - val_loss: 18.6013\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 17.31557\n",
            "Epoch 403/2000\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 14.8673 - val_loss: 18.4035\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 17.31557\n",
            "Epoch 404/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.2932 - val_loss: 17.6153\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 17.31557\n",
            "Epoch 405/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 14.8701 - val_loss: 18.5940\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 17.31557\n",
            "Epoch 406/2000\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 15.8832 - val_loss: 19.4636\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 17.31557\n",
            "Epoch 407/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.8428 - val_loss: 18.7943\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 17.31557\n",
            "Epoch 408/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 15.2177 - val_loss: 17.9320\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 17.31557\n",
            "Epoch 409/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 14.7268 - val_loss: 18.4626\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 17.31557\n",
            "Epoch 410/2000\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 14.3949 - val_loss: 17.5387\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 17.31557\n",
            "          loss   val_loss\n",
            "405  15.883181  19.463638\n",
            "406  15.842784  18.794342\n",
            "407  15.217750  17.931992\n",
            "408  14.726785  18.462597\n",
            "409  14.394877  17.538694\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "실제가격: 22.600, 예상가격: 24.781\n",
            "실제가격: 50.000, 예상가격: 26.943\n",
            "실제가격: 23.000, 예상가격: 22.736\n",
            "실제가격: 8.300, 예상가격: 10.709\n",
            "실제가격: 21.200, 예상가격: 18.845\n",
            "실제가격: 19.900, 예상가격: 21.533\n",
            "실제가격: 20.600, 예상가격: 23.070\n",
            "실제가격: 18.700, 예상가격: 22.658\n",
            "실제가격: 16.100, 예상가격: 17.157\n",
            "실제가격: 18.600, 예상가격: 10.397\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffg_9rdJDUPd"
      },
      "source": [
        "#주식 마감 주가 예측\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/\"\n",
        "data= pd.read_csv(path+\"data-02-stock_daily.csv\", header=1)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize = (30, 20))\n",
        "ax1= fig.add_subplot(3, 1, 1)\n",
        "ax2 = fig.add_subplot(3, 1, 2)\n",
        "ax3 = fig.add_subplot(3, 1, 3)\n",
        "\n",
        "ax1.plot(data['Open'])\n",
        "ax1.plot(data['High'])\n",
        "ax1.plot(data['Low'])\n",
        "ax1.plot(data['Close'])\n",
        "ax1.plot(data['Volume'])\n",
        "ax3.plot(data['Open'][0:7], linewidth=3.0, label=\"Open\")\n",
        "ax3.plot(data['High'][0:7], linewidth=3.0, label=\"High\")\n",
        "ax3.plot(data['Low'][0:7], linewidth=3.0, label=\"Low\")\n",
        "ax3.plot(data['Close'][0:7], linewidth=3.0, label=\"Close\")\n",
        "\n",
        "\n",
        "ax3.legend(prop={'size':30})\n",
        "\n",
        "xdata =data[[\"Open\", \"High\", \"Low\", \"Volume\"]]\n",
        "ydata = pd.DataFrame(data[\"Close\"])\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "xdata_ss = StandardScaler().fit_transform(xdata)\n",
        "ydata_ss = StandardScaler().fit_transform(ydata)\n",
        "\n",
        "print(xdata_ss.shape, ydata_ss.shape)\n",
        "\n",
        "#트레이닝 테스트 데이터 분리\n",
        "xtrain=xdata_ss[220:,:]\n",
        "xtest=xdata_ss[:220,:]\n",
        "ytrain=ydata_ss[220:,:]\n",
        "ytest=ydata_ss[:220,:]\n",
        "\n",
        "#xtrain,xtest, ytrain, ytest = train_test_split(xdata_ss, ydata_ss, test_size=0.1, random_state=seed)\n",
        "print(xtrain.shape, ytrain.shape, xtest.shape, ytest.shape)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(units = 1024, input_dim=4, activation='relu'))\n",
        "model.add(Dense(units = 512, activation='relu'))\n",
        "model.add(Dense(units = 256, activation='relu'))\n",
        "model.add(Dense(units = 128, activation='relu'))\n",
        "model.add(Dense(units = 64, activation='relu'))\n",
        "model.add(Dense(units = 32, activation='relu'))\n",
        "model.add(Dense(units=1)) \n",
        "\n",
        "model.compile(loss = 'mse', optimizer='adam', metrics=['mae'])\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "es = EarlyStopping(monitor = \"mae\", patience=10)\n",
        "\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "hist = model.fit(xtrain, ytrain, epochs=100, batch_size= 16, callbacks=[es])\n",
        "\n",
        "print(\"loss:\"+str(hist.history['loss']))\n",
        "print(\"MAE:\"+str(hist.history['mae']))\n",
        "\n",
        "res = model.evaluate(xtest, ytest, batch_size=32)\n",
        "print(\"loss\", res[0], \"mae\", res[1])\n",
        "\n",
        "xhat = xtest\n",
        "yhat=model.predict(xhat)\n",
        "plt.figure()\n",
        "plt.plot(yhat, label=\"predicted\")\n",
        "plt.plot(ytest, label=\"actual\")\n",
        "plt.legend(prop={'size':20})\n",
        "print(\"Evaluate: {}\".format(np.average((yhat-ytest)**2)))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T2LKgQYVRff_",
        "outputId": "79e081a8-b4df-47f2-83c4-5250bd84c930"
      },
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "samples = iris.data\n",
        "print(samples)\n",
        "\n",
        "#두가지 feature(특성)만을 사용\n",
        "from matplotlib import pyplot as plt\n",
        "x = samples[:, 0]\n",
        "y = samples[:,1]\n",
        "plt.scatter(x,y,alpha=0.5) #색상의 투명도 ; 0-완전투명, 1-불투명\n",
        "plt.xlabel('sepal length(cm)')\n",
        "plt.ylabel('sepal width(cm)')\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.2]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.6 1.4 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [6.2 3.4 5.4 2.3]\n",
            " [5.9 3.  5.1 1.8]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "XIZuq19Ss08N",
        "outputId": "3e311a04-cbca-4f1b-e8e8-3bf053986036"
      },
      "source": [
        "#Place K radom Centroids: 제일 먼저 k개의 centroids(중심값)를 임의로 지정\n",
        "#3가지 종이 존재하므로 k는 3으로 설정\n",
        "\n",
        "import numpy as np\n",
        "k = 3\n",
        "\n",
        "#랜덤으로x,y좌표 3개를 생성합니다\n",
        "#nprandom.uniform은 주어진 최소, 최대값 사이에서 k개만큼 실수, 난수를 생성합니다\n",
        "centroids_x = np.random.uniform(min(x), max(x), k)\n",
        "centroids_y = np.random.uniform(min(y), max(y), k)\n",
        "centroids = list(zip(centroids_x, centroids_y))\n",
        "print(centroids)\n",
        "\n",
        "#centrodis는 임의로 생성한 (x, y)좌표 3개를 갖게 됨\n",
        "plt.scatter(x, y, alpha=0.5) #데이터들은 파란색으로 표시\n",
        "plt.scatter(centroids_x, centroids_y) #centroids는 주황색으로 표시\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(6.855704730863831, 3.8877344982051767), (4.418921492075821, 2.7465080129841137), (5.368926799179885, 3.4767625633995225)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8bn5cNSyUFj"
      },
      "source": [
        "def distandce(a, b):\n",
        "  reurtn sum([(el_a = el_b)**2 for el_a, el_b in list(zop(a,b))]) ** 0.5\n",
        "\n",
        "  labels - np.zeros(len(samples))\n",
        "  sepal_length_width = np.array(list(zip(x,y)))\n",
        "\n",
        "  for i in range(len(samples)):\n",
        "    distaces = np.zeros(k)\n",
        "    for j in range(len(k)):\n",
        "      distances[j] = distance(sepal_length_width[i], centroids[j])\n",
        "    cluster = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K6bZ0HiyT_7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}